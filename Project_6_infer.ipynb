{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18748b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import ast\n",
    "import random\n",
    "import builtins\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score,f1_score\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33762cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:    ['i', 'know', 'that', 'as', 'teenagers', 'we', 'all', 'have', 'the', 'desire', 'to', 'have', 'cell', 'phones', 'and', 'use', 'them', '.', 'i', 'think', 'that', 'we', 'should', \"n't\", 'be', 'able', 'to', 'have', 'cell', 'phones', 'in', 'school', '.', 'i', 'believe', 'this', 'because', 'cell', 'phones', 'are', 'only', 'objects', 'we', 'do', \"n't\", 'need', 'to', 'have', 'we', 'just', 'want', 'to', '.', 'i', 'wo', 'nt', 'lie', 'i', 'love', 'my', 'cell', 'phone', 'and', 'like', 'to', 'talk', 'and', 'text', 'to', 'my', 'friends', 'all', 'the', 'time', 'but', 'i', 'think', 'that', 'we', 'should', 'have', 'a', 'limit', 'to', 'how', 'much', 'use', 'we', 'get', 'out', 'of', 'them', '.', 'if', 'we', 'bring', 'them', 'to', 'school', 'it', 'will', 'only', 'increase', 'the', 'amount', 'of', 'people', 'who', 'just', 'slack', 'off', 'and', 'do', \"n't\", 'do', 'work', '.', 'my', 'father', 'told', 'me', 'that', 'having', 'a', 'cell', 'phone', 'is', 'a', 'privilege', 'not', 'a', 'given', 'right', 'that', 'everyone', 'has', '.', 'i', 'know', 'that', 'there', \"'s\", 'already', 'so', 'many', 'bad', 'things', 'happening', 'in', 'school', 'like', 'bullying', 'and', 'people', 'breaking', 'the', 'rules', 'but', 'i', 'know', 'what', 'is', 'right', 'and', 'what', 'is', 'wrong', '.', 'people', 'in', 'school', 'who', 'just', 'do', 'whatever', 'they', 'want', 'and', 'do', \"n't\", 'expect', 'anything', 'to', 'happen', 'to', 'them', 'never', 'win', '.', 'i', 'see', 'tons', 'of', 'people', 'in', 'my', 'school', 'all', 'the', 'time', 'with', 'cell', 'phones', 'and', 'there', 'not', 'supposed', 'to', 'have', 'them', 'but', 'they', 'still', 'do', '.', 'if', 'we', 'allow', 'kids', 'to', 'have', 'cell', 'phones', 'in', 'school', 'they', 'will', 'become', 'lazy', 'in', 'there', 'work', 'and', 'not', 'try', 'they', 'will', 'start', 'thinking', 'they', 'are', 'above', 'the', 'rules', 'and', 'above', 'punishment', '.', 'i', 'stand', 'on', 'the', 'ground', 'that', 'technology', 'will', 'corrupt', 'people', 'if', 'we', 'do', \"n't\", 'set', '<UNK>', 'lines', 'for', 'it', '.', 'i', 'know', 'that', 'since', 'we', 'have', 'gotten', 'more', 'sophisticated', 'technology', 'kids', 'are', 'getting', 'more', 'and', 'more', 'lazy', 'and', 'mean', '.', 'i', 'know', 'that', 'not', 'all', 'technology', 'is', 'bad', 'but', 'i', 'think', 'we', 'can', 'do', 'without', 'some', 'of', 'it', '.', 'i', 'think', 'if', 'we', 'had', '<UNK>', 'rules', 'and', 'better', 'morals', 'kids', 'would', 'be', 'a', 'lot', 'better', 'than', 'we', 'are', 'now', '.', 'our', 'parents', 'are', 'teaching', 'us', 'the', 'things', 'that', 'we', 'need', 'to', 'know', 'but', 'they', 'had', 'much', '<UNK>', 'lives', 'than', 'us', 'and', 'if', 'we', 'do', \"n't\", 'listen', 'how', 'can', 'we', 'learn', '.', 'we', 'have', 'so', 'many', 'bad', 'influences', 'in', 'our', 'life', 'time', 'like', 'tv', 'stars', 'and', 'rock', 'stars', 'that', 'we', 'try', 'and', '<UNK>', 'and', 'they', 'usually', 'end', 'up', 'making', 'bad', 'choices', 'so', 'if', 'we', 'try', 'to', 'be', 'like', 'them', 'we', 'will', 'to', '.', 'in', 'our', 'society', 'we', 'are', 'cruel', 'we', 'judge', 'people', 'solely', 'on', 'looks', 'and', 'talent', 'and', 'if', 'they', 'have', 'neither', 'they', 'are', '<UNK>', '<UNK>', '.', 'i', 'think', 'if', 'we', 'took', 'the', 'time', 'to', 'understand', 'people', 'we', 'could', 'all', 'get', 'along', '.', 'if', 'i', 'have', 'children', 'i', 'want', 'there', 'lives', 'to', 'be', 'better', 'than', 'mine', 'i', 'want', 'to', 'teach', 'them', 'not', 'to', 'be', 'lazy', 'or', 'mean', 'i', 'want', 'to', 'help', 'them', 'succeed', 'where', 'i', 'have', 'failed', '.', 'if', 'we', 'continue', 'on', 'the', 'route', 'our', 'society', 'is', 'taking', 'it', 'will', 'only', 'end', 'us', 'up', 'in', 'a', 'terrible', 'place', 'where', 'we', 'should', \"n't\", 'be', '.']\n",
      "Predicted Labels:   ['lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'position', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'claim', 'position', 'claim', 'claim', 'lead', 'claim', 'claim', 'claim', 'lead', 'lead', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'evidence', 'claim', 'evidence', 'claim', 'claim', 'evidence', 'claim', 'claim', 'claim', 'claim', 'evidence', 'claim', 'evidence', 'evidence', 'evidence', 'claim', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'concluding', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'concluding', 'evidence', 'evidence', 'evidence', 'evidence', 'concluding', 'evidence', 'evidence', 'evidence', 'evidence', 'concluding', 'concluding', 'evidence', 'evidence', 'concluding', 'concluding', 'concluding', 'concluding', 'evidence', 'evidence', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding']\n",
      "Ground Truth Labels: ['lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'position', 'O', 'O', 'O', 'O', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'O', 'O', 'O', 'O', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'counterclaim', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'rebuttal', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'claim', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'evidence', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding', 'concluding']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import ast\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Constants\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 641\n",
    "EMBEDDING_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "FF_DIM = 256\n",
    "NUM_TRANSFORMER_BLOCKS = 10\n",
    "DROPOUT_RATE = 0.1\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "\n",
    "# Load and prepare test data\n",
    "df = pd.read_csv(\"classification.csv\")\n",
    "essay = {'tokens': [], 'labels': []}\n",
    "for _, row in df.iterrows():\n",
    "    ess = ast.literal_eval(row['tokens'])\n",
    "    lab = ast.literal_eval(row['labels'])\n",
    "    essay['tokens'].append(ess)\n",
    "    essay['labels'].append(lab)\n",
    "df = pd.DataFrame(essay)\n",
    "\n",
    "# Label mappings (these must match training)\n",
    "all_labels_list = [label for sublist in df['labels'] for label in sublist]\n",
    "label_list = sorted(list(set(all_labels_list)))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "PAD_LABEL_ID = label2id.get('O', 0)\n",
    "\n",
    "# Token vocab\n",
    "all_tokens = [token.lower() for sublist in df['tokens'] for token in sublist]\n",
    "word_counts = pd.Series(all_tokens).value_counts()\n",
    "vocab = word_counts.head(VOCAB_SIZE - 2).index.tolist()\n",
    "word2id = {word: i + 2 for i, word in enumerate(vocab)}\n",
    "word2id[PAD_TOKEN] = 0\n",
    "word2id[UNK_TOKEN] = 1\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "ACTUAL_VOCAB_SIZE = len(word2id)\n",
    "PAD_TOKEN_ID = word2id[PAD_TOKEN]\n",
    "\n",
    "# Prepare input\n",
    "X = [[word2id.get(token.lower(), word2id[UNK_TOKEN]) for token in seq] for seq in df['tokens']]\n",
    "y = [[label2id[label] for label in seq] for seq in df['labels']]\n",
    "X_padded = np.array([seq[:MAX_LEN] + [PAD_TOKEN_ID] * (MAX_LEN - len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN] for seq in X])\n",
    "y_padded = np.array([seq[:MAX_LEN] + [PAD_LABEL_ID] * (MAX_LEN - len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN] for seq in y])\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Model definition (must match training)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim))\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        x_norm = self.layernorm1(x)\n",
    "        attn_output, _ = self.att(x_norm, x_norm, x_norm, key_padding_mask=padding_mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x_norm = self.layernorm2(x)\n",
    "        x = x + self.dropout2(self.ffn(x_norm))\n",
    "        return x\n",
    "\n",
    "class TokenClassifierTransformer(nn.Module):\n",
    "    def __init__(self, num_transformer_blocks, embed_dim, num_heads, ff_dim, vocab_size, num_labels, max_len, dropout_rate, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embed_dim, num_labels)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        padding_mask = (input_ids == self.padding_idx)\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, padding_mask)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Load model\n",
    "# model = TokenClassifierTransformer(\n",
    "#     num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "#     embed_dim=EMBEDDING_DIM,\n",
    "#     num_heads=NUM_HEADS,\n",
    "#     ff_dim=FF_DIM,\n",
    "#     vocab_size=ACTUAL_VOCAB_SIZE,\n",
    "#     num_labels=len(label2id),\n",
    "#     max_len=MAX_LEN,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "#     padding_idx=PAD_TOKEN_ID\n",
    "# ).to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"ak.pth\"),map_location='cpu')\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\") # You already defined this, which is good\n",
    "\n",
    "model = TokenClassifierTransformer(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    vocab_size=ACTUAL_VOCAB_SIZE,\n",
    "    num_labels=len(label2id),\n",
    "    max_len=MAX_LEN,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    padding_idx=PAD_TOKEN_ID\n",
    ").to(device) # Ensure model is created on the target device first\n",
    "\n",
    "checkpoint = torch.load(\"classification.pth\", map_location=device)\n",
    "\n",
    "if isinstance(checkpoint, OrderedDict) or isinstance(checkpoint, dict):\n",
    "    if 'model_state_dict' in checkpoint: # Check for common nesting\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif 'state_dict' in checkpoint: # Another common key\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    model.load_state_dict(state_dict)\n",
    "elif isinstance(checkpoint, nn.Module):\n",
    "     print(\"Warning: Loaded entire model object, extracting state_dict.\")\n",
    "     model.load_state_dict(checkpoint.state_dict())\n",
    "else:\n",
    "    raise TypeError(f\"Loaded checkpoint is of unexpected type: {type(checkpoint)}\")\n",
    "\n",
    "\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "\n",
    "# Predict on one sample\n",
    "idx = 4\n",
    "input_ids = torch.LongTensor(X_padded[idx:idx+1]).to(device)\n",
    "gt_labels = y_padded[idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "pred_ids = torch.argmax(logits[0], dim=-1).cpu().numpy()\n",
    "input_ids_np = input_ids[0].cpu().numpy()\n",
    "\n",
    "original_tokens = []\n",
    "predicted_labels = []\n",
    "ground_truth_labels = []\n",
    "\n",
    "for token_id, pred_id, gt_id in zip(input_ids_np, pred_ids, gt_labels):\n",
    "    if token_id == PAD_TOKEN_ID:\n",
    "        break\n",
    "    original_tokens.append(id2word.get(token_id, '<UNK>'))\n",
    "    predicted_labels.append(id2label.get(pred_id, 'UNK'))\n",
    "    ground_truth_labels.append(id2label.get(gt_id, 'UNK'))\n",
    "\n",
    "print(\"Original Tokens:   \", original_tokens)\n",
    "print(\"Predicted Labels:  \", (predicted_labels))\n",
    "print(\"Ground Truth Labels:\",( ground_truth_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ebbc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "    def __init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act=\"gelu\",hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,layer_norm_eps=1e-12,pad_token_id=0):\n",
    "        self.vocab_size=vocab_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_hidden_layers=num_hidden_layers\n",
    "        self.num_attention_heads=num_attention_heads\n",
    "        self.intermediate_size=intermediate_size\n",
    "        self.hidden_act=hidden_act\n",
    "        self.hidden_dropout_prob=hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob=attention_probs_dropout_prob\n",
    "        self.max_position_embeddings=max_position_embeddings\n",
    "        self.type_vocab_size=type_vocab_size\n",
    "        self.layer_norm_eps=layer_norm_eps\n",
    "        self.pad_token_id=pad_token_id\n",
    "        if num_attention_heads>0 and hidden_size%num_attention_heads==0:self.attention_head_size=hidden_size//num_attention_heads\n",
    "        else:self.attention_head_size=hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3612469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings=nn.Embedding(config.vocab_size,config.hidden_size,padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings=nn.Embedding(config.max_position_embeddings,config.hidden_size)\n",
    "        self.token_type_embeddings=nn.Embedding(config.type_vocab_size,config.hidden_size)\n",
    "        self.LayerNorm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "        self.dropout=nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.register_buffer(\"position_ids\",torch.arange(config.max_position_embeddings).expand((1,-1)))\n",
    "        self.position_embedding_type=getattr(config,\"position_embedding_type\",\"absolute\")\n",
    "\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids=None):\n",
    "        seq_length=input_ids.size(1)\n",
    "        if token_type_ids is None: token_type_ids=torch.zeros_like(input_ids)\n",
    "        word_embeds=self.word_embeddings(input_ids)\n",
    "        token_type_embeds=self.token_type_embeddings(token_type_ids)\n",
    "        position_ids=self.position_ids[:,:seq_length].to(input_ids.device)\n",
    "        position_embeds=self.position_embeddings(position_ids)\n",
    "        embeddings=word_embeds+position_embeds+token_type_embeds\n",
    "        embeddings=self.LayerNorm(embeddings)\n",
    "        embeddings=self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06eaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads=config.num_attention_heads\n",
    "        self.attention_head_size=config.attention_head_size\n",
    "        self.all_head_size=self.num_attention_heads*self.attention_head_size\n",
    "        self.query=nn.Linear(config.hidden_size,self.all_head_size)\n",
    "        self.key=nn.Linear(config.hidden_size,self.all_head_size)\n",
    "        self.value=nn.Linear(config.hidden_size,self.all_head_size)\n",
    "        self.dropout=nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self,x):\n",
    "        new_x_shape=x.size()[:-1]+(self.num_attention_heads,self.attention_head_size)\n",
    "        x=x.view(*new_x_shape)\n",
    "        return x.permute(0,2,1,3)\n",
    "\n",
    "    def forward(self,hidden_states,attention_mask=None):\n",
    "        query_layer=self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer=self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer=self.transpose_for_scores(self.value(hidden_states))\n",
    "        attention_scores=torch.matmul(query_layer,key_layer.transpose(-1,-2))/math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "          attention_scores=attention_scores+attention_mask\n",
    "        attention_probs=nn.functional.softmax(attention_scores,dim=-1)\n",
    "        attention_probs=self.dropout(attention_probs)\n",
    "        context_layer=torch.matmul(attention_probs,value_layer)\n",
    "        context_layer=context_layer.permute(0,2,1,3).contiguous()\n",
    "        new_context_layer_shape=context_layer.size()[:-2]+(self.all_head_size,)\n",
    "        context_layer=context_layer.view(*new_context_layer_shape)\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99fddacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttentionOutput(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__(); self.dense=nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.LayerNorm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "        self.dropout=nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self,hidden_states,input_tensor):\n",
    "        hidden_states=self.dropout(self.dense(hidden_states))\n",
    "        hidden_states=self.LayerNorm(hidden_states+input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3ccfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "      super().__init__()\n",
    "      self.self=BertSelfAttention(config)\n",
    "      self.output=BertAttentionOutput(config)\n",
    "    def forward(self,hidden_states,attention_mask=None):\n",
    "      return self.output(self.self(hidden_states,attention_mask),hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64af3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name):\n",
    "    if name == \"gelu\": return nn.GELU()\n",
    "    if name == \"relu\": return nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71effc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self,config):\n",
    "      super().__init__()\n",
    "      self.dense=nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "      self.intermediate_act_fn=get_activation(config.hidden_act)\n",
    "    def forward(self,hidden_states):\n",
    "      return self.intermediate_act_fn(self.dense(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbbac1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    def __init__(self,config):\n",
    "      super().__init__()\n",
    "      self.dense=nn.Linear(config.intermediate_size,config.hidden_size)\n",
    "      self.LayerNorm=nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n",
    "      self.dropout=nn.Dropout(config.hidden_dropout_prob)\n",
    "    def forward(self,hidden_states,input_tensor):\n",
    "      hidden_states=self.dropout(self.dense(hidden_states))\n",
    "      hidden_states=self.LayerNorm(hidden_states+input_tensor)\n",
    "      return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c1c4a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self,config):\n",
    "      super().__init__()\n",
    "      self.attention=BertAttention(config)\n",
    "      self.intermediate=BertIntermediate(config)\n",
    "      self.output=BertOutput(config)\n",
    "    def forward(self,hidden_states,attention_mask=None):\n",
    "      attention_output=self.attention(hidden_states,attention_mask)\n",
    "      intermediate_output=self.intermediate(attention_output)\n",
    "      layer_output=self.output(intermediate_output,attention_output)\n",
    "      return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da28aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "      super().__init__()\n",
    "      self.layer=nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "    def forward(self,hidden_states,attention_mask=None):\n",
    "        for layer_module in self.layer:\n",
    "          hidden_states=layer_module(hidden_states,attention_mask)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0131efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self,config):\n",
    "      super().__init__()\n",
    "      self.config=config\n",
    "      self.embeddings=BertEmbeddings(config)\n",
    "      self.encoder=BertEncoder(config)\n",
    "    def get_input_embeddings(self):\n",
    "      return self.embeddings.word_embeddings\n",
    "    def set_input_embeddings(self,value):\n",
    "      self.embeddings.word_embeddings=value\n",
    "    def _prune_heads(self,heads_to_prune):\n",
    "      raise NotImplementedError\n",
    "    def forward(self,input_ids,attention_mask=None,token_type_ids=None):\n",
    "        if attention_mask is None:\n",
    "          attention_mask=torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "          token_type_ids=torch.zeros_like(input_ids)\n",
    "        extended_attention_mask=attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask=extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask=(1.0-extended_attention_mask)*-10000.0\n",
    "        embedding_output=self.embeddings(input_ids=input_ids,token_type_ids=token_type_ids)\n",
    "        sequence_output=self.encoder(embedding_output,attention_mask=extended_attention_mask)\n",
    "        return {\"last_hidden_state\": sequence_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54680138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab_limit=5000):\n",
    "        self.word_to_id={}\n",
    "        self.id_to_word={}\n",
    "        self.vocab_limit=vocab_limit\n",
    "        self.pad_token=\"[PAD]\"\n",
    "        self.unk_token=\"[UNK]\"\n",
    "        self.cls_token=\"[CLS]\"\n",
    "        self.sep_token=\"[SEP]\"\n",
    "        self.special_tokens=[self.pad_token,self.unk_token,self.cls_token,self.sep_token]\n",
    "        for i,token in enumerate(self.special_tokens):\n",
    "            self.word_to_id[token]=i\n",
    "            self.id_to_word[i]=token\n",
    "        self.vocab_built=False\n",
    "\n",
    "    def _basic_tokenize(self,text):\n",
    "        text=text.lower()\n",
    "        tokens=re.findall(r\"[\\w']+|[.,!?;]\",text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self,corpus_records):\n",
    "        if self.vocab_built: return\n",
    "        print(f\"Building vocabulary from {len(corpus_records)} records...\")\n",
    "        word_counts=Counter()\n",
    "        for record in tqdm(corpus_records,desc=\"Processing Corpus\"):\n",
    "            segments=record.get(\"segments\",{})\n",
    "            for dt in segments:\n",
    "                for text_segment in segments[dt]:\n",
    "                    tokens=self._basic_tokenize(text_segment)\n",
    "                    word_counts.update(tokens)\n",
    "        current_id=len(self.special_tokens)\n",
    "        sorted_words=sorted(word_counts.items(),key=lambda item: item[1],reverse=True)\n",
    "        for word,count in tqdm(sorted_words,desc=\"Adding words to vocab\"):\n",
    "            if word not in self.word_to_id:\n",
    "                if current_id >= self.vocab_limit: break\n",
    "                self.word_to_id[word]=current_id\n",
    "                self.id_to_word[current_id]=word\n",
    "                current_id += 1\n",
    "        self.vocab_size=len(self.word_to_id)\n",
    "        self.pad_token_id=self.word_to_id[self.pad_token]\n",
    "        self.unk_token_id=self.word_to_id[self.unk_token]\n",
    "        self.cls_token_id=self.word_to_id[self.cls_token]\n",
    "        self.sep_token_id=self.word_to_id[self.sep_token]\n",
    "        self.vocab_built=True\n",
    "        print(f\"Vocabulary building complete. Size: {self.vocab_size}\")\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        return self._basic_tokenize(text)\n",
    "\n",
    "    def convert_tokens_to_ids(self,tokens):\n",
    "        return [self.word_to_id.get(token,self.unk_token_id) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self,ids):\n",
    "        return [self.id_to_word.get(id,self.unk_token) for id in ids]\n",
    "\n",
    "    def _pad(self,ids,max_length):\n",
    "        padding_len=max_length-len(ids)\n",
    "        return ids+[self.pad_token_id]*padding_len if padding_len>0 else ids\n",
    "\n",
    "    def _truncate(self,ids,max_length):\n",
    "        effective_max_len=max_length-2\n",
    "        return ids[:effective_max_len] if len(ids)>effective_max_len else ids\n",
    "\n",
    "    def encode_plus(self,text,add_special_tokens=True,max_length=None,padding='max_length',truncation=True,return_tensors=None,return_attention_mask=True):\n",
    "        tokens=self.tokenize(text)\n",
    "        if truncation and max_length is not None:\n",
    "          tokens=self._truncate(tokens,max_length)\n",
    "        ids=self.convert_tokens_to_ids(tokens)\n",
    "        if add_special_tokens:\n",
    "          ids=[self.cls_token_id]+ids+[self.sep_token_id]\n",
    "        attention_mask=[1]*len(ids)\n",
    "        if padding=='max_length' and max_length is not None:\n",
    "            pad_len=max_length-len(ids)\n",
    "            if pad_len>0:\n",
    "              ids=ids+[self.pad_token_id]*pad_len; attention_mask=attention_mask+[0]*pad_len\n",
    "            elif pad_len < 0:\n",
    "              ids=ids[:max_length]; attention_mask=attention_mask[:max_length]\n",
    "        output={\"input_ids\":ids}\n",
    "        if return_attention_mask:\n",
    "          output[\"attention_mask\"]=attention_mask\n",
    "        if return_tensors==\"pt\":\n",
    "            for key in output: output[key]=torch.tensor(output[key])\n",
    "        return output\n",
    "\n",
    "    def __call__(self,text_batch,**kwargs):\n",
    "         if isinstance(text_batch,builtins.str):\n",
    "             return self.encode_plus(text_batch,**kwargs)\n",
    "         elif isinstance(text_batch,builtins.list):\n",
    "             batch_outputs=[self.encode_plus(text,**kwargs) for text in text_batch]\n",
    "             if not batch_outputs:\n",
    "                 print(\"Warning: Empty batch encountered in SimpleTokenizer.__call__\")\n",
    "                 return {}\n",
    "             collated={}\n",
    "             keys=batch_outputs[0].keys()\n",
    "             for key in keys:\n",
    "                 if kwargs.get(\"return_tensors\") == \"pt\":\n",
    "                     items_to_stack=[item[key] for item in batch_outputs if isinstance(item.get(key),torch.Tensor)]\n",
    "                     if len(items_to_stack) == len(batch_outputs):\n",
    "                         collated[key]=torch.stack(items_to_stack)\n",
    "                     else:\n",
    "                         print(f\"Warning: Could not stack tensors for key '{key}' due to inconsistent types.\")\n",
    "                         collated[key]=[item.get(key) for item in batch_outputs]\n",
    "                 else:\n",
    "                     collated[key]=[item[key] for item in batch_outputs]\n",
    "             return collated\n",
    "         else:\n",
    "             raise TypeError(f\"Input must be a string or a list of strings,got {type(text_batch)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd6f4101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1000 essays.\n",
      "Maximum segment token length found: 641\n",
      "Score columns being used: ['lead', 'position', 'claim', 'rebuttal', 'evidence', 'concluding', 'counterclaim']\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(rf'classification.csv')\n",
    "df1=pd.read_csv(rf'score.csv')\n",
    "df1.set_index('essay_id', inplace=True)\n",
    "df1.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "import ast\n",
    "essay=[]\n",
    "mx=0\n",
    "for i in df.iterrows():\n",
    "    dic={\"segments\":{\"lead\":[],\"position\":[],\"claim\":[],\"rebuttal\":[],\"evidence\":[],\"concluding\":[],\"counterclaim\":[]},\"score\":[]}\n",
    "    row=list((df1.loc[i[1]['essay_id']]).to_numpy())\n",
    "    ess=ast.literal_eval(i[1]['tokens'])\n",
    "    lab=ast.literal_eval(i[1]['labels'])\n",
    "    assert (len(ess) == len(lab))\n",
    "    j=0\n",
    "    while j < len(ess):\n",
    "        str=\"\"\n",
    "        cur=lab[j]\n",
    "        lenght=0\n",
    "        while j < len(ess) and lab[j] == cur:\n",
    "            if str!=\"\":\n",
    "                str+=\" \"\n",
    "            str+=ess[j]\n",
    "            lenght+=1\n",
    "            j+=1\n",
    "        mx=max(mx,lenght)\n",
    "        if cur == \"O\":\n",
    "            continue\n",
    "        dic['segments'][cur].append(str)\n",
    "    dic['score']=row\n",
    "    essay.append(dic)\n",
    "essay_data_list=essay\n",
    "print(f\"Successfully processed {len(essay_data_list)} essays.\")\n",
    "print(f\"Maximum segment token length found: {mx}\")\n",
    "MAX_LEN_COLLATE=128\n",
    "discourse_types=[\"lead\", \"position\", \"claim\", \"rebuttal\", \"evidence\", \"concluding\", \"counterclaim\"]\n",
    "score_columns=df1.columns.tolist()\n",
    "print(f\"Score columns being used: {score_columns}\")\n",
    "MAX_LEN_COLLATE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc37b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, records, tokenizer): self.records = records\n",
    "    def __len__(self): return len(self.records)\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]; scores = [float(s) for s in rec[\"score\"]]\n",
    "        return {\"segments\": rec[\"segments\"], \"labels\": torch.tensor(scores, dtype=torch.float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "141f8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch,tokenizer_instance,max_len):\n",
    "    labels=torch.stack([b[\"labels\"] for b in batch],dim=0)\n",
    "    batched_text={dt: [] for dt in discourse_types}\n",
    "    for b in batch:\n",
    "        for dt in discourse_types:\n",
    "            segs=b[\"segments\"].get(dt,[])\n",
    "            batched_text[dt].append(\" \".join(segs) if segs else tokenizer_instance.pad_token)\n",
    "    tokenized={}\n",
    "    for dt in discourse_types:\n",
    "        tokenized[dt]=tokenizer_instance(batched_text[dt],padding='max_length',truncation=True,max_length=max_len,return_tensors=\"pt\",add_special_tokens=True,return_attention_mask=True)\n",
    "    return tokenized,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "372a28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEInteraction(nn.Module):\n",
    "    def __init__(self,d_model=768,n_experts=len(discourse_types),k=2):\n",
    "        super().__init__()\n",
    "        self.gate=nn.Linear(d_model,n_experts)\n",
    "        self.experts=nn.ModuleList([nn.Sequential(nn.Linear(d_model,d_model*4),nn.GELU(),nn.Linear(d_model*4,d_model)) for _ in range(n_experts)])\n",
    "        self.k=k\n",
    "        self.n_experts=n_experts\n",
    "\n",
    "    def forward(self,E):\n",
    "        if E.shape[0] != self.n_experts:\n",
    "          return torch.zeros_like(E)\n",
    "        gate_logits=self.gate(E)\n",
    "        topk_vals,topk_inds=torch.topk(gate_logits,self.k,dim=-1)\n",
    "        out=torch.zeros_like(E)\n",
    "        for i in range(E.size(0)):\n",
    "            weights=torch.softmax(topk_vals[i],dim=-1)\n",
    "            expert_outputs=[]\n",
    "            for expert_index,weight in zip(topk_inds[i],weights):\n",
    "                if expert_index < self.n_experts: expert_outputs.append(weight.unsqueeze(0)*self.experts[expert_index](E[i]))\n",
    "            if expert_outputs: out[i]=torch.stack(expert_outputs).sum(dim=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bd98923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalMoEScorer(nn.Module):\n",
    "    def __init__(self,bert_scratch_config,n_experts=len(discourse_types),k=2,num_labels=len(score_columns)):\n",
    "        super().__init__()\n",
    "        self.bert=BertModel(bert_scratch_config)\n",
    "        bert_hidden_size=bert_scratch_config.hidden_size\n",
    "        self.moe=MoEInteraction(d_model=bert_hidden_size,n_experts=n_experts,k=k)\n",
    "        self.heads=nn.ModuleList([nn.Linear(bert_hidden_size,1) for _ in range(num_labels)])\n",
    "        self.discourse_types=discourse_types; self.num_labels=num_labels\n",
    "        self.config=bert_scratch_config\n",
    "\n",
    "    def forward(self,tokenized_batch):\n",
    "        B=tokenized_batch[self.discourse_types[0]][\"input_ids\"].size(0)\n",
    "        device=next(self.parameters()).device\n",
    "        bert_hidden_size=self.config.hidden_size\n",
    "        disc_embeddings=[]\n",
    "        for dt in self.discourse_types:\n",
    "            ids=tokenized_batch[dt][\"input_ids\"].to(device)\n",
    "            mask=tokenized_batch[dt][\"attention_mask\"].to(device)\n",
    "            last_hidden_state=self.bert(input_ids=ids,attention_mask=mask)[\"last_hidden_state\"]\n",
    "            disc_embeddings.append(last_hidden_state[:,0,:])\n",
    "\n",
    "        E=torch.stack(disc_embeddings,dim=1)\n",
    "        H_all=[]\n",
    "        for i in range(B):\n",
    "          H_all.append(self.moe(E[i]))\n",
    "        H=torch.stack(H_all,dim=0)\n",
    "        pooled_H=H.mean(dim=1)\n",
    "        outs=[self.heads[j](pooled_H).squeeze(-1) for j in range(self.num_labels)]\n",
    "        final_preds=torch.stack(outs,dim=1)\n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc13c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: 800 train,200 validation records.\n",
      "Building vocabulary from 800 records...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5425adb4356464281bb74a20b606777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Corpus:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1559d48d9b44516b9719c9d86bac235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding words to vocab:   0%|          | 0/10556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary building complete. Size: 10560\n",
      "Vocab Size: 10560\n",
      " BERT Config: vocab=10560,hidden=768,layers=6,pad_id=0\n"
     ]
    }
   ],
   "source": [
    "train_records,val_records=train_test_split(essay_data_list,test_size=0.2,random_state=42)\n",
    "print(f\"Data split: {len(train_records)} train,{len(val_records)} validation records.\")\n",
    "\n",
    "tokenizer=SimpleTokenizer(vocab_limit=12000)\n",
    "tokenizer.build_vocab(train_records)\n",
    "print(f\"Vocab Size: {tokenizer.vocab_size}\")\n",
    "\n",
    "train_data=EssayDataset(train_records,tokenizer)\n",
    "val_data=EssayDataset(val_records,tokenizer)\n",
    "collate_fn_with_args=partial(collate_fn,tokenizer_instance=tokenizer,max_len=MAX_LEN_COLLATE)\n",
    "train_loader=DataLoader(train_data,batch_size=4,shuffle=True,collate_fn=collate_fn_with_args,num_workers=0)\n",
    "val_loader=DataLoader(val_data,batch_size=4,shuffle=False,collate_fn=collate_fn_with_args,num_workers=0)\n",
    "\n",
    "\n",
    "bert_config=BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=MAX_LEN_COLLATE,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    "  )\n",
    "print(f\" BERT Config: vocab={bert_config.vocab_size},hidden={bert_config.hidden_size},layers={bert_config.num_hidden_layers},pad_id={bert_config.pad_token_id}\")\n",
    "model=HierarchicalMoEScorer(bert_scratch_config=bert_config,n_experts=len(discourse_types),k=2,num_labels=len(score_columns) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7604361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232d512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7195a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
